<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="SVM on MNISTSVMSVM is a supervised machine learning model used for classification and regression.Here, we gonna use it as a classifier on MNIST, a han">
    

    <!--Author-->
    
        <meta name="author" content="Sheng-Tang Wong">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="SVM"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="new"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>SVM - new</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/website/public/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
	
    <!-- Loading mathjax macro -->
    <!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/website/public/">$</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/website/public/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/website/public/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/website/public/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/website/public/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/stw09701125/">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('./source/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>SVM</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2017-12-27
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->

<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           

            <!-- Gallery -->
            
             
                    <div id="toc" class="post-toc-content col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                        <ol class="nav-toc"><li class="nav-toc-item nav-toc-level-1"><a class="nav-toc-link" href="#SVM-on-MNIST"><span class="nav-toc-number">1.</span> <span class="nav-toc-text">SVM on MNIST</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#SVM"><span class="nav-toc-number">1.1.</span> <span class="nav-toc-text">SVM</span></a></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#LIBSVM"><span class="nav-toc-number">1.2.</span> <span class="nav-toc-text">LIBSVM</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Introduction"><span class="nav-toc-number">1.2.1.</span> <span class="nav-toc-text">Introduction</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#How-to-Use"><span class="nav-toc-number">1.2.2.</span> <span class="nav-toc-text">How to Use</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#How-to-Use-in-python"><span class="nav-toc-number">1.2.3.</span> <span class="nav-toc-text">How to Use in python</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#PCA"><span class="nav-toc-number">1.3.</span> <span class="nav-toc-text">PCA</span></a></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#SVM-in-Python-with-LIBSVM"><span class="nav-toc-number">1.4.</span> <span class="nav-toc-text">SVM in Python with LIBSVM</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Data-Preprocessing"><span class="nav-toc-number">1.4.1.</span> <span class="nav-toc-text">Data Preprocessing</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#main"><span class="nav-toc-number">1.4.2.</span> <span class="nav-toc-text">main</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#with-Different-Kernel"><span class="nav-toc-number">1.4.3.</span> <span class="nav-toc-text">with Different Kernel</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#Result"><span class="nav-toc-number">1.5.</span> <span class="nav-toc-text">Result</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Showing-Support-Vector"><span class="nav-toc-number">1.5.1.</span> <span class="nav-toc-text">Showing Support Vector</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Showing-Decision-Boundary"><span class="nav-toc-number">1.5.2.</span> <span class="nav-toc-text">Showing Decision Boundary</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#Finding-Optimum-Parameter-set"><span class="nav-toc-number">1.6.</span> <span class="nav-toc-text">Finding Optimum Parameter set</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#grid-search-approach"><span class="nav-toc-number">1.6.1.</span> <span class="nav-toc-text">grid search approach</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#grid-py"><span class="nav-toc-number">1.6.2.</span> <span class="nav-toc-text">grid.py</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#code-of-PCA-first-method"><span class="nav-toc-number">1.7.</span> <span class="nav-toc-text">code of PCA first method</span></a></li></ol></li></ol>
                    </div>
            
            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
               
                <h1 id="SVM-on-MNIST"><a href="#SVM-on-MNIST" class="headerlink" title="SVM on MNIST"></a>SVM on MNIST</h1><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>SVM is a supervised machine learning model used for classification and regression.<br>Here, we gonna use it as a classifier on MNIST, a handwritten data set.<br>Our goal is to build a set of hyperplanes with the largest margin that seperates the classes.</p>
<blockquote>
<p><em>H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin.</em><br><em>from wiki - Support vector machine</em>
<img src="./SVM.png" align="middle" weight="256" height="221" title="SVM_eg"> </p>
</blockquote>
<h2 id="LIBSVM"><a href="#LIBSVM" class="headerlink" title="LIBSVM"></a>LIBSVM</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>LIBSVM is a libray for Support Vector Machines constructed by professor Chih-Jen Lin.<br>From its <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" rel="noopener">website</a> says  </p>
<blockquote>
<p><em>&quot;<strong>LIBSVM</strong> is an integrated software for support vector classification (C-SVC, nu-SVC), regression (epsilon-SVR, nu-SVR) and distribution estimation (one-class SVM). It supports multi-class classification.&quot;</em></p>
</blockquote>
<p>According to this, We can tell LIBSVM is a powerful tool.<br>And here we&#39;ll use C-SVC to build our model for handwritten classification.  </p>
<p><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf" target="_blank" rel="noopener">Tutorial for beginner</a>  </p>
<h3 id="How-to-Use"><a href="#How-to-Use" class="headerlink" title="How to Use"></a>How to Use</h3><p>LIBSVM also provides a great number of programming language interfaces, including Python, R, Matlab, Perl, Ruby, etc.<br>Therefore, we gonna use it in python.  </p>
<p>First, we have to change our raw data into libsvm input format.  </p>
<blockquote>
<p><em>Format of input data</em></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">label1 index1:value1 index2:value2 ... indexN:valueN</span><br><span class="line">label2 index1:value1...</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br></pre></td></tr></table></figure>
</blockquote>
<hr>
<p><a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/breast-cancer_scale" target="_blank" rel="noopener">breast cancer example(scaled)</a></p>
<p>Second, scale data value into the same scale range.<br>Third, select the proper kernel fit our data set.<br>And then find the best parameter for our model.<br>Finally, we test our model with test data set.  </p>
<hr>
<blockquote>
<p><strong>Recommended steps from &quot;Tutorial&quot; for beginner: </strong>  </p>
<ol>
<li><span id="1">Transform data to the format of an SVM package.</span></li>
<li>Conduct simple scaling on the data.</li>
<li>Consider the RBF kernel $K(x, y) = e^{-\gamma||x - y||^2}$  </li>
<li>Use cross-validation to find the best parameter C and $\gamma$  </li>
<li>Use the best parameter C and $\gamma$ to train the whole training set</li>
<li>Test</li>
</ol>
</blockquote>
<h3 id="How-to-Use-in-python"><a href="#How-to-Use-in-python" class="headerlink" title="How to Use in python"></a>How to Use in python</h3><p>Check their <a href="https://github.com/cjlin1/libsvm" target="_blank" rel="noopener">github</a> to download the source code.<br>After that, go to libsvm and python folder making file.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">STWMacBook: ~/libsvm% make</span><br><span class="line">STWMacBook: ~/libsvm/python% make</span><br></pre></td></tr></table></figure>
<p>We can just import libsvm and call the functions of libsvm in our python code.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> svmutil <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<p><strong>Caution: </strong>We have to move &quot;svm.py&quot;, &quot;svmutil.py&quot;, &quot;libsvm.so.2&quot; and &quot;svm-train&quot; into the folder where our python code is.</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>Every data of MINST is a image with 28X28 pixels, that is, a 784-dimensional data point.<br>Therefore, we have to reduce its dimention, here we use PCA as our dimensionality reduction approach.  </p>
<p>We are not ganna explain why PCA work, we just have to know that we pick few dimensions that can represent our data well and the rest of them are not that important.</p>
<p><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">More about PCA</a>  </p>
<hr>
<p>Using PCA from sklearn help us visualizing our data set.<br>And then plot them out.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Applying PCA</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components = <span class="number">2</span>)</span><br><span class="line">X_train_pca = pca.fit_transform(X_train)</span><br><span class="line">X_test_pca = pca.fit_transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(dataset, labels, titlename=<span class="string">''</span>)</span>:</span></span><br><span class="line">    colors = [<span class="string">"skyblue"</span>, <span class="string">"lightgray"</span>, <span class="string">"greenyellow"</span>, <span class="string">"salmon"</span>, <span class="string">"magenta"</span>]</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        plt.scatter(dataset[labels[:, <span class="number">0</span>] == index, <span class="number">0</span>], dataset[labels[:, <span class="number">0</span>] == index, <span class="number">1</span>], \</span><br><span class="line">        s = <span class="number">30</span>, c = colors[index - <span class="number">1</span>], label = <span class="string">'Cluster '</span> + str(index))</span><br><span class="line">    plt.title(<span class="string">'PCA of MNIST '</span> + titlename)</span><br><span class="line">    plt.xlabel(<span class="string">'PC1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'PC2'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">plot(X_train_pca, Y_train, <span class="string">'training data'</span>)</span><br><span class="line">plot(X_test_pca, Y_test, <span class="string">'testing data'</span>)</span><br></pre></td></tr></table></figure>
<div>
    <img src="./PCA_of_testing_data.png" align="left" weight="194" height="139" title="testpca"> 
    <img src="./PCA_of_training_data.png" align="right|top" weight="194" height="139" title="trainpca">
</div>



<h2 id="SVM-in-Python-with-LIBSVM"><a href="#SVM-in-Python-with-LIBSVM" class="headerlink" title="SVM in Python with LIBSVM"></a>SVM in Python with LIBSVM</h2><h3 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h3><p><a href="#1">First step</a>, load data, change format of it and output as text file.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">Train_data = pd.read_csv(<span class="string">'X_train.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line">Train_label = pd.read_csv(<span class="string">'T_train.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line">Test_data = pd.read_csv(<span class="string">'X_test.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line">Test_label = pd.read_csv(<span class="string">'T_test.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">X_train = Train_data.iloc[:, :].values</span><br><span class="line">Y_train = Train_label.iloc[:].values</span><br><span class="line">X_test = Test_data.iloc[:, :].values</span><br><span class="line">Y_test = Test_label.iloc[:].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># training set</span></span><br><span class="line"><span class="comment"># changing format</span></span><br><span class="line">svm_format_train = np.concatenate((Y_train, X_train), axis=<span class="number">1</span>)</span><br><span class="line">svm_format_train = svm_format_train.astype(str)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> svm_format_train:</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">785</span>):</span><br><span class="line">        item[idx] = str(idx) + <span class="string">":"</span> + item[idx]</span><br><span class="line"><span class="comment"># output as text file</span></span><br><span class="line">np.savetxt(<span class="string">"svm_format_train"</span>, svm_format_train, fmt=<span class="string">"%s"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test set</span></span><br><span class="line"><span class="comment"># changing format</span></span><br><span class="line">svm_format_test = np.concatenate((Y_test, X_test), axis=<span class="number">1</span>)</span><br><span class="line">svm_format_test = svm_format_test.astype(str)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> svm_format_test:</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">785</span>):</span><br><span class="line">        item[idx] = str(idx) + <span class="string">":"</span> + item[idx]</span><br><span class="line"><span class="comment"># output as text file</span></span><br><span class="line">np.savetxt(<span class="string">"svm_format_test"</span>, svm_format_test, fmt=<span class="string">"%s"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><p>Train our model with libsvm.<br>Since, our raw data has already scaled, we just need to start training.<br>The default kernel is RBF.</p>
<hr>
<p>-t kernel_type : set type of kernel function (default 2)<br>    0 -- linear: u&#39;<em>v<br>    1 -- polynomial: (gamma</em>u&#39;<em>v + coef0)^degree<br>    2 -- radial basis function: exp(-gamma</em>|u-v|^2)<br>    3 -- sigmoid: tanh(gamma<em>u&#39;</em>v + coef0)</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svm_train(y, x, <span class="string">"-t 2 -c 8 -g 0.03125"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> svmutil <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">y, x = svm_read_problem(<span class="string">"svm_format_train"</span>)</span><br><span class="line">    </span><br><span class="line">model = svm_train(y, x, <span class="string">"-c 8 -g 0.03125"</span>)</span><br><span class="line"></span><br><span class="line">y_t, x_t = svm_read_problem(<span class="string">"svm_format_test"</span>)</span><br><span class="line"></span><br><span class="line">p_label, p_acc, p_val = svm_predict(y_t, x_t, model)</span><br><span class="line"></span><br><span class="line">pca2 = PCA(n_components = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">X_test_pca2 = pca2.fit_transform(X_test)</span><br><span class="line">    </span><br><span class="line">plot(X_test_pca2, np.array(p_label).reshape(<span class="number">-1</span>, <span class="number">1</span>), <span class="string">'predictive data'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="with-Different-Kernel"><a href="#with-Different-Kernel" class="headerlink" title="with Different Kernel"></a>with Different Kernel</h3><p>Try different kernel and see what is gonna happen.    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = svm_train(y, x, <span class="string">"-t 1 -r 5 -d 3 -c 8 -g 0.03125"</span>)</span><br><span class="line">p_label, p_acc, p_val = svm_predict(y_t, x_t, model)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = svm_train(y, x, <span class="string">"-t 3 -r 5 -c 8 -g 0.03125"</span>)</span><br><span class="line">p_label, p_acc, p_val = svm_predict(y_t, x_t, model)</span><br></pre></td></tr></table></figure>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>We plotted the result of RBF kernel svm model with 784-dimentional data points by PCA into 2 significant dimensions, just for visualization.  </p>
<blockquote>
<p><em>The result with accuracy of 98.52% (2463/2500)</em></p>
<div>
  <img src="./aftertrain_PCA2_9852.png" align="middle" weight="194" height="139" title="testpca"><br>  <img src="./PCA_of_testing_data.png" align="middle" weight="194" height="139" title="testpca">
</div>

</blockquote>
<p>We found that the result of prediction was <strong>really close</strong> to groundtruth, almost the same.</p>
<p>However, we have an alternative option that we can do PCA first and use selected dimensions to do model training.<br><a href="#code">Check here for code</a>  </p>
<blockquote>
<p><em>The result with accuracy of 74.48% (1862/2500)</em></p>
<p><img src="./afterPCA2_train_7448.png" align="middle" weight="194" height="139" title="testpca">  </p>
</blockquote>
<p><strong>The result did not look well, but was good for visualization, when we did PCA first. </strong> </p>
<p><strong>Therefore, we used this on showing support vector and decision boundary.</strong></p>
<h3 id="Showing-Support-Vector"><a href="#Showing-Support-Vector" class="headerlink" title="Showing Support Vector"></a>Showing Support Vector</h3><p>After model was trained, we could get support vectors from the svm_model structure of LIBSVM.   </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">svm_model</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">svm_parameter</span> <span class="title">param</span>;</span>	<span class="comment">/* parameter */</span></span><br><span class="line">	<span class="keyword">int</span> nr_class;	<span class="comment">/* number of classes, = 2 in regression/one class svm */</span></span><br><span class="line">	<span class="keyword">int</span> l;		<span class="comment">/* total #SV */</span></span><br><span class="line">	<span class="class"><span class="keyword">struct</span> <span class="title">svm_node</span> **<span class="title">SV</span>;</span>	<span class="comment">/* SVs (SV[l]) */</span></span><br><span class="line">	<span class="keyword">double</span> **sv_coef;<span class="comment">/* coefficients for SVs in decision functions (sv_coef[k-1][l]) */</span></span><br><span class="line">	<span class="keyword">double</span> *rho;	<span class="comment">/* constants in decision functions (rho[k*(k-1)/2]) */</span></span><br><span class="line">	<span class="keyword">double</span> *probA;	<span class="comment">/* pariwise probability information */</span></span><br><span class="line">	<span class="keyword">double</span> *probB;</span><br><span class="line">	<span class="keyword">int</span> *sv_indices; </span><br><span class="line">    <span class="comment">/* sv_indices[0,...,nSV-1] are values in [1,...,num_traning_data] to indicate SVs in the training set */</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">/* for classification only */</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> *label;	<span class="comment">/* label of each class (label[k]) */</span></span><br><span class="line">	<span class="keyword">int</span> *nSV;	<span class="comment">/* number of SVs for each class (nSV[k]) */</span></span><br><span class="line">                <span class="comment">/* nSV[0] + nSV[1] + ... + nSV[k-1] = l */</span></span><br><span class="line">	<span class="comment">/* XXX */</span></span><br><span class="line">	<span class="keyword">int</span> free_sv;<span class="comment">/* 1 if svm_model is created by svm_load_model*/</span></span><br><span class="line">				<span class="comment">/* 0 if svm_model is created by svm_train */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_support_vector</span><span class="params">(model, training_set)</span>:</span></span><br><span class="line">    nSV = [] <span class="comment"># number of support vectors of each label</span></span><br><span class="line">    SVs = [] <span class="comment"># index of each support vector in trainging data</span></span><br><span class="line">    support_v = [] <span class="comment"># the support vectors</span></span><br><span class="line">    begin = <span class="number">0</span></span><br><span class="line">    end = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store number of support vectors</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        nSV.append(model.nSV[i])</span><br><span class="line">    <span class="comment"># find out the index of each support vector in training data </span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nSV:</span><br><span class="line">        temp = []</span><br><span class="line">        end += num</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(begin, end):</span><br><span class="line">            temp.append(model.sv_indices[idx] - <span class="number">1</span>)</span><br><span class="line">        begin = end</span><br><span class="line">        SVs.append(temp)</span><br><span class="line">    <span class="comment"># select every support vector in dataset </span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> SVs:</span><br><span class="line">        support_v.append(training_set[item])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> support_v</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_SV</span><span class="params">(dataset, labels, SV, titlename=<span class="string">''</span>)</span>:</span></span><br><span class="line">    colors = [<span class="string">"skyblue"</span>, <span class="string">"lightgray"</span>, <span class="string">"greenyellow"</span>, <span class="string">"salmon"</span>, <span class="string">"khaki"</span>]</span><br><span class="line">    colors_v = [<span class="string">"rebeccapurple"</span>, <span class="string">"red"</span>, <span class="string">"k"</span>, <span class="string">"b"</span>, <span class="string">"limegreen"</span>]</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        plt.scatter(dataset[labels[:, <span class="number">0</span>] == index, <span class="number">0</span>], dataset[labels[:, <span class="number">0</span>] == index, <span class="number">1</span>], s = <span class="number">30</span>, c = colors[index - <span class="number">1</span>], label = <span class="string">'Cluster '</span> + str(index))</span><br><span class="line">        plt.scatter(SV[index - <span class="number">1</span>][<span class="number">0</span>:<span class="number">10</span>, <span class="number">0</span>], SV[index - <span class="number">1</span>][<span class="number">0</span>:<span class="number">10</span>, <span class="number">1</span>], s = <span class="number">100</span>, c = colors_v[index - <span class="number">1</span>], marker=(<span class="number">5</span>, <span class="number">1</span>))</span><br><span class="line">    plt.title(<span class="string">'PCA of MNIST '</span> + titlename)</span><br><span class="line">    plt.xlabel(<span class="string">'PC1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'PC2'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># To get support vectors and then plot them out</span></span><br><span class="line">SVs = get_support_vector(model, X_train_pca)</span><br><span class="line">plot_with_SV(X_test_pca, np.array(p_label).reshape(<span class="number">-1</span>, <span class="number">1</span>), SVs, <span class="string">'predictive data'</span>)</span><br></pre></td></tr></table></figure>
<p>Note that we plotted only 10 support vectors of each class, because there were too many support vectors which cannot be visualized well.  </p>
<blockquote>
<p><em>graph with support vectors ($\star$ stars)</em></p>
<p><img src="./afterPCA_train_with_support_vector.png" align="middle" weight="194" height="139" title="support_vector"> </p>
</blockquote>
<h3 id="Showing-Decision-Boundary"><a href="#Showing-Decision-Boundary" class="headerlink" title="Showing Decision Boundary"></a>Showing Decision Boundary</h3><p>We used SVC from sklearn.svm to train our model for simplicity, due to the approach for plotting decision boundary.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">classifier = SVC(kernel = <span class="string">'rbf'</span>, random_state = <span class="number">0</span>)</span><br><span class="line">classifier.fit(X_train_pca, Y_train.reshape(Y_train.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">y_pred = classifier.predict(X_test_pca)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">X_set, Y_set = X_test_pca, Y_test.reshape(Y_test.shape[<span class="number">0</span>])</span><br><span class="line">X1, X2 = np.meshgrid(np.arange(start = X_set[:, <span class="number">0</span>].min() - <span class="number">1</span>, stop = X_set[:, <span class="number">0</span>].max() + <span class="number">1</span>, step = <span class="number">0.01</span>),</span><br><span class="line">                     np.arange(start = X_set[:, <span class="number">1</span>].min() - <span class="number">1</span>, stop = X_set[:, <span class="number">1</span>].max() + <span class="number">1</span>, step = <span class="number">0.01</span>))</span><br><span class="line">plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),</span><br><span class="line">             alpha = <span class="number">0.8</span>, cmap = plt.cm.rainbow)</span><br><span class="line"><span class="comment">#ListedColormap(('skyblue', 'snow', 'greenyellow', 'salmon', 'khaki'))</span></span><br><span class="line"><span class="comment">#snow-&gt;darkviolet</span></span><br><span class="line"><span class="comment">#ListedColormap(('royalblue', 'rebeccapurple', 'limegreen', 'red', 'sandybrown'))</span></span><br><span class="line">plt.xlim(X1.min(), X1.max())</span><br><span class="line">plt.ylim(X2.min(), X2.max())</span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> enumerate(np.unique(Y_set)):</span><br><span class="line">    plt.scatter(X_set[Y_set == j, <span class="number">0</span>], X_set[Y_set == j, <span class="number">1</span>],</span><br><span class="line">                c = ListedColormap((<span class="string">'rebeccapurple'</span>, <span class="string">'royalblue'</span>, <span class="string">'limegreen'</span>, <span class="string">'sandybrown'</span>, <span class="string">'red'</span>))(i), label = j)</span><br><span class="line">plt.title(<span class="string">'SVM (Testing set)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'PC1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'PC2'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>How did we plot those prediction regions or boundaries?  </p>
<p>We took all the pixels of the frame, resolved them into 0.01 and applied our classifier on it.  </p>
<p>That is, for each new point(pixel of 0.01 resolution), our model would classify them,<br>and colorize them in the color that they belong.  </p>
<p>After doing this on all the pixel points in the frame,<br>we could see that those pixel points formed the decision regions or boundaries.  </p>
<blockquote>
<p><em>decision boundary</em></p>
<p><img src="./decision_boundary3.png" align="middle" weight="194" height="139" title="support_vector"></p>
</blockquote>
<h2 id="Finding-Optimum-Parameter-set"><a href="#Finding-Optimum-Parameter-set" class="headerlink" title="Finding Optimum Parameter set"></a>Finding Optimum Parameter set</h2><p>In our MNIST case, we use rbf kernel for our svm model, so we have to decide the value of two parameters C and $\gamma$.  </p>
<p>But what value should we choose that would give us the best result?</p>
<p>There are several ways to find optimum parameter set, like grid search,<br>Bayesian optimization, random search, gradient-based optimization, etc.</p>
<p>Here we use grid search approach.</p>
<h3 id="grid-search-approach"><a href="#grid-search-approach" class="headerlink" title="grid search approach"></a>grid search approach</h3><p>This approach is pretty straightforward.  </p>
<p>Grid search is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search" target="_blank" rel="noopener">wiki-Hyperparameter_optimization</a></p>
<p>Why we choose the simplest way?</p>
<p>There are few reasons, says from <em><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf" target="_blank" rel="noopener">Guide page 6</a></em> :  </p>
<ol>
<li>We may not feel safe to use methods which avoid doing an exhaustive parameter search by approximations or heuristics.  </li>
<li>The computational time required to find good parameters by grid search is not much more than that by advanced methods since there are only two parameters.  </li>
<li>The grid search can be easily parallelized because each $(C, \gamma)$ is independent.  </li>
</ol>
<h3 id="grid-py"><a href="#grid-py" class="headerlink" title="grid.py"></a>grid.py</h3><p>LIBSVM provide a tool to help us finding the best parameters, using grid search approach, called grid.py.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%run -i grid.py svm_format_train</span><br></pre></td></tr></table></figure>
<p>We found that the best parameter set is $(C, \gamma) = (2^3, 2^{-5})$ </p>
<hr>
<h2 id="code-of-PCA-first-method"><a href="#code-of-PCA-first-method" class="headerlink" title="code of PCA first method"></a><strong><span id="code">code of PCA first method</span></strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Sat Dec 23 16:25:23 2017</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: stw</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">Train_data = pd.read_csv(<span class="string">'X_train.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line">Train_label = pd.read_csv(<span class="string">'T_train.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line">Test_data = pd.read_csv(<span class="string">'X_test.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line">Test_label = pd.read_csv(<span class="string">'T_test.csv'</span>, header = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">X_train = Train_data.iloc[:, :].values</span><br><span class="line">Y_train = Train_label.iloc[:].values</span><br><span class="line">X_test = Test_data.iloc[:, :].values</span><br><span class="line">Y_test = Test_label.iloc[:].values</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(dataset, labels, titlename=<span class="string">''</span>)</span>:</span></span><br><span class="line">    colors = [<span class="string">"skyblue"</span>, <span class="string">"lightgray"</span>, <span class="string">"greenyellow"</span>, <span class="string">"salmon"</span>, <span class="string">"khaki"</span>]</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        plt.scatter(dataset[labels[:, <span class="number">0</span>] == index, <span class="number">0</span>], dataset[labels[:, <span class="number">0</span>] == index, <span class="number">1</span>], s = <span class="number">30</span>, c = colors[index - <span class="number">1</span>], label = <span class="string">'Cluster '</span> + str(index))</span><br><span class="line">    plt.title(<span class="string">'PCA of MNIST '</span> + titlename)</span><br><span class="line">    plt.xlabel(<span class="string">'PC1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'PC2'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_format</span><span class="params">(X, Y, dimension, name)</span>:</span></span><br><span class="line">    svm_format = np.concatenate((Y, X), axis=<span class="number">1</span>)</span><br><span class="line">    svm_format = svm_format.astype(str)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> svm_format:</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">1</span>, dimension + <span class="number">1</span>):</span><br><span class="line">            item[idx] = str(idx) + <span class="string">":"</span> + item[idx]</span><br><span class="line">    np.savetxt(name, svm_format, fmt=<span class="string">"%s"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Applying PCA</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">    </span><br><span class="line">D = <span class="number">2</span></span><br><span class="line">pca = PCA(D)</span><br><span class="line">X_train_pca = pca.fit_transform(X_train)</span><br><span class="line">X_test_pca = pca.fit_transform(X_test)</span><br><span class="line">change_format(X_train_pca, Y_train, D, <span class="string">"svm_format_train"</span>)</span><br><span class="line">change_format(X_test_pca, Y_test, D, <span class="string">"svm_format_test"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> svmutil <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">y, x = svm_read_problem(<span class="string">"svm_format_train"</span>)</span><br><span class="line">model = svm_train(y, x, <span class="string">"-c 2 -g 0.125"</span>)</span><br><span class="line"></span><br><span class="line">y_t, x_t = svm_read_problem(<span class="string">"svm_format_test"</span>)</span><br><span class="line"></span><br><span class="line">p_label, p_acc, p_val = svm_predict(y_t, x_t, model)</span><br><span class="line"></span><br><span class="line">plot(X_test_pca, np.array(p_label).reshape(<span class="number">-1</span>, <span class="number">1</span>), <span class="string">'predictive data'</span>)</span><br></pre></td></tr></table></figure>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

<!-- <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div> -->

 <!-- <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
         <div class="post-toc"> -->

			<!-- 
				<div id="toc" class="post-toc-content">
					<ol class="nav-toc"><li class="nav-toc-item nav-toc-level-1"><a class="nav-toc-link" href="#SVM-on-MNIST"><span class="nav-toc-number">1.</span> <span class="nav-toc-text">SVM on MNIST</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#SVM"><span class="nav-toc-number">1.1.</span> <span class="nav-toc-text">SVM</span></a></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#LIBSVM"><span class="nav-toc-number">1.2.</span> <span class="nav-toc-text">LIBSVM</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Introduction"><span class="nav-toc-number">1.2.1.</span> <span class="nav-toc-text">Introduction</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#How-to-Use"><span class="nav-toc-number">1.2.2.</span> <span class="nav-toc-text">How to Use</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#How-to-Use-in-python"><span class="nav-toc-number">1.2.3.</span> <span class="nav-toc-text">How to Use in python</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#PCA"><span class="nav-toc-number">1.3.</span> <span class="nav-toc-text">PCA</span></a></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#SVM-in-Python-with-LIBSVM"><span class="nav-toc-number">1.4.</span> <span class="nav-toc-text">SVM in Python with LIBSVM</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Data-Preprocessing"><span class="nav-toc-number">1.4.1.</span> <span class="nav-toc-text">Data Preprocessing</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#main"><span class="nav-toc-number">1.4.2.</span> <span class="nav-toc-text">main</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#with-Different-Kernel"><span class="nav-toc-number">1.4.3.</span> <span class="nav-toc-text">with Different Kernel</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#Result"><span class="nav-toc-number">1.5.</span> <span class="nav-toc-text">Result</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Showing-Support-Vector"><span class="nav-toc-number">1.5.1.</span> <span class="nav-toc-text">Showing Support Vector</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#Showing-Decision-Boundary"><span class="nav-toc-number">1.5.2.</span> <span class="nav-toc-text">Showing Decision Boundary</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#Finding-Optimum-Parameter-set"><span class="nav-toc-number">1.6.</span> <span class="nav-toc-text">Finding Optimum Parameter set</span></a><ol class="nav-toc-child"><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#grid-search-approach"><span class="nav-toc-number">1.6.1.</span> <span class="nav-toc-text">grid search approach</span></a></li><li class="nav-toc-item nav-toc-level-3"><a class="nav-toc-link" href="#grid-py"><span class="nav-toc-number">1.6.2.</span> <span class="nav-toc-text">grid.py</span></a></li></ol></li><li class="nav-toc-item nav-toc-level-2"><a class="nav-toc-link" href="#code-of-PCA-first-method"><span class="nav-toc-number">1.7.</span> <span class="nav-toc-text">code of PCA first method</span></a></li></ol></li></ol>
				</div>
			 -->
 <!--     </section>
    </div>
  </aside> -->


    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    
                        <li>
                            <a href="https://stw09701125.github.io/website/" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-home fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        

                    
                        <li>
                            <a href="https://www.facebook.com/shengtang.wong" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    
                        <li>
                            <a href="https://github.com/stw09701125/" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    
                        <li>
                            <a href="https://www.linkedin.com/in/sheng-tang-wong-305931128/" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2017 Sheng-Tang Wong<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->


<!-- Sidebar motions -->
<!-- <script src="/website/public/js/motion.js"></script>
<script src="/website/public/js/velocity.min.js"></script>
<script src="/website/public/js/velocity.ui.min.js"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>

</html>